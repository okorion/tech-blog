---
title: "ANN 기본기: 왜 뉴런·레이어·활성화·손실부터 시작해야 하는가"
description: "뉴런·활성화·손실·역전파로 ANN 학습 골격을 잡고 딥러닝 전 모델의 공통 원리를 정리"
categories: ["🧬 Deep Learning"]
tags: [ANN, Backpropagation, Activation, Loss]
image: /assets/posts/2025-12-15-deep-learning/image.jpg
date: 2025-12-15 21:50:00 +09:00
last_modified_at: 2025-12-15 21:50:00 +09:00
---

# ANN 기본기: 왜 뉴런·레이어·활성화·손실부터 시작해야 하는가

---

## TL;DR

* ANN은 **모든 딥러닝 모델의 학습 메커니즘 원형**이다. CNN/RNN도 본질은 같다.
* 핵심은 구조가 아니라 **손실 함수 → 경사하강 → 역전파**의 흐름이다.
* 활성화 함수는 “비선형성”보다 **학습 안정성** 관점에서 선택해야 한다.
* 역전파는 수식 암기가 아니라 **계산 그래프에서의 책임 전파**로 이해해야 한다.
* 고객 이탈 예측 같은 **표(tabular) 데이터 문제에서 ANN은 여전히 최적 선택**이다.
* ANN이 안 되는 이유의 80%는 **데이터/손실/스케일링/학습률** 문제다.
* ANN을 제대로 이해하면, 이후 모델들은 “구조만 다른 같은 게임”으로 보인다.

---

## 전체 지도: ANN은 딥러닝에서 어떤 위치인가

**한 문장 요약:** ANN은 딥러닝의 “알파이자 오메가”, 모든 신경망의 공통 학습 골격이다.

딥러닝 모델 계보를 단순화하면 다음과 같다.

```
ANN (학습의 기본 골격)
 ├─ CNN (공간 구조 반영)
 ├─ RNN/LSTM (시간 구조 반영)
 ├─ AutoEncoder (표현 학습)
 └─ 기타 모든 신경망
```

* CNN/RNN은 **ANN의 뉴런·가중치·손실·역전파를 그대로 사용**한다.
* 차이는 “입력을 어떻게 연결하느냐(구조)”뿐이다.
* 따라서 ANN을 이해하지 못하면, 이후 모델은 **암기 대상**이 된다.

**미니 사례:**
고객 이탈 예측 → ANN
이미지 분류 → CNN이 필요
하지만 **학습이 되는 이유**는 둘 다 ANN의 학습 원리를 쓰기 때문이다.

---

## 핵심 개념: 왜 ANN이 필요한가, 무엇인가

### 1) 뉴런(Neuron)

**한 문장 요약:** 뉴런은 “입력을 가중합 → 비선형 변환 → 출력”하는 계산 단위다.

* 정의:
  입력 특징들에 가중치를 곱해 합한 뒤, 활성화 함수를 통과시킨다.
* 직관:
  “여러 기준을 가중 평균 내고, 기준을 넘으면 반응한다.”
* 왜 필요한가:
  선형 조합만으로는 복잡한 패턴을 표현할 수 없기 때문.

**비유:**
뉴런은 “의사결정자”다.
여러 신호를 보고 중요도를 다르게 반영해 판단한다.

---

### 2) 레이어(Layer)

**한 문장 요약:** 레이어는 “특징을 점점 추상화하는 단계적 변환”이다.

* 입력층: 원본 데이터
* 은닉층: 문제 해결에 유용한 중간 표현
* 출력층: 우리가 원하는 예측

**왜 여러 층이 필요한가:**
단일 뉴런은 단순 기준선밖에 못 만든다.
층을 쌓으면 **복합 규칙**을 표현할 수 있다.

**미니 사례:**
고객 이탈 예측

* 1층: 나이, 사용 기간, 요금제 조합
* 2층: “불만족 고객 패턴”
* 출력층: 이탈 확률

---

### 3) 활성화 함수(Activation Function)

**한 문장 요약:** 활성화 함수는 신경망에 **비선형성과 학습 안정성**을 부여한다.

#### 활성화 함수 선택 기준 (문제/안정성 관점)

| 함수    | 목적           | 언제 쓰나            | 주의점                        |
| ------- | -------------- | -------------------- | ----------------------------- |
| Sigmoid | 확률 출력      | 이진 분류 출력층     | 기울기 소실, 은닉층 사용 금지 |
| Tanh    | 중심 0 비선형  | RNN 일부 구조        | 여전히 기울기 소실            |
| ReLU    | 학습 안정/속도 | **은닉층 기본 선택** | Dead ReLU 가능                |

**핵심 포인트**

* 은닉층: **ReLU 계열이 기본**
* 출력층: 문제 유형에 따라 선택

  * 이진 분류 → Sigmoid
  * 다중 분류 → Softmax

**오해 주의:**
“비선형성”보다 **기울기가 잘 흐르느냐**가 더 중요하다.

---

### 4) 손실 함수(Loss Function)

**한 문장 요약:** 손실 함수는 “모델이 얼마나 틀렸는지”를 숫자로 정의한다.

* 분류: Binary Cross Entropy, Categorical Cross Entropy
* 회귀: MSE, MAE

**왜 중요한가:**
모델은 손실을 줄이는 방향으로만 움직인다.
손실 선택이 곧 **문제 정의**다.

**미니 사례:**
이탈 예측에서 MSE를 쓰면
→ “확률이 맞는지”보다 “숫자 차이”에 집착
→ 분류 성능이 망가진다.

---

## 작동 원리: ANN은 어떻게 동작하고 학습하는가

### 1) 입력 → 출력 (Forward Pass)

**한 문장 요약:** 입력을 통과시켜 예측을 만든다.

```
입력 X
 → (가중합 + 활성화)
 → 은닉층들
 → 출력 ŷ
```

이 단계에서는 **아직 학습이 일어나지 않는다**.

---

### 2) 학습의 핵심: 역전파(Backpropagation)

**한 문장 요약:** 역전파는 “누가 얼마나 잘못했는지 책임을 거꾸로 나누는 과정”이다.

#### 계산 그래프 관점의 직관

```
입력 → 연산1 → 연산2 → 출력 → 손실
```

* 손실이 커졌다
  → 출력에 가까운 연산이 1차 책임
  → 그 영향이 이전 연산으로 연쇄 전달

즉,

* 각 가중치는 “내가 손실 증가에 얼마나 기여했는가”를 계산받는다.
* 그 기여도만큼 **조금씩 수정**된다.

**중요:**

* 수식을 외우는 게 아니라
* **“손실의 원인을 거슬러 올라간다”**고 이해하면 충분하다.

---

### 3) 경사하강법(GD / SGD)

**한 문장 요약:** 경사하강법은 “손실이 줄어드는 방향으로 조금씩 이동”하는 전략이다.

* GD: 전체 데이터 기준 (느림)
* SGD: 일부 샘플 기준 (빠름, 노이즈 있음)
* 실무: **Mini-batch SGD**

**비유:**
안개 낀 산에서 아래로 내려가는 법
→ 발밑 기울기만 보고 조금씩 이동

---

## 구현 체크리스트 (ANN 기준)

### 데이터

* [ ] 타깃 불균형 여부 확인(이탈 문제의 핵심)
* [ ] 입력 스케일링(Standard/MinMax)
* [ ] Train/Validation 분리

### 모델

* [ ] 은닉층 ReLU
* [ ] 출력층 활성화 = 문제 유형과 일치
* [ ] 파라미터 수 과도하지 않은지

### 학습

* [ ] 손실 감소 확인
* [ ] 학습률 너무 크지 않은지
* [ ] Early Stopping 설정

### 평가

* [ ] Accuracy 단독 사용 금지
* [ ] AUC / F1 병행

### 디버깅

* [ ] 입력/출력 shape 확인
* [ ] 라벨 인코딩 오류 여부
* [ ] 손실 함수와 출력층 조합 점검

---

## 실전 적용: 고객 이탈 예측에서 ANN 쓰는 이유

**한 문장 요약:** 고객 이탈은 “복합적인 표 데이터 패턴” 문제다.

* ANN 장점

  * 피처 간 비선형 관계 학습
  * 피처 엔지니어링 부담 감소
* 핵심 지표

  * AUC, Recall (놓치면 손실 큼)
* 리스크

  * 불균형 데이터
  * 임계값(threshold) 설정 실수

**실무 포인트:**
“확률 예측”과 “의사결정 임계값”은 분리해서 관리해야 한다.

---

## 흔한 함정 TOP 7 + 해결책

1. **손실 함수와 출력층 불일치**
   → 해결: 이진 분류 = Sigmoid + BCE

2. **은닉층에 Sigmoid 사용**
   → 해결: ReLU로 교체

3. **스케일링 안 된 입력**
   → 해결: 항상 스케일링 먼저

4. **Accuracy만 보고 판단**
   → 해결: AUC/F1 필수

5. **학습률 과대**
   → 해결: 1e-3부터 시작

6. **데이터 불균형 무시**
   → 해결: class weight / resampling

7. **모델이 안 되면 구조부터 바꿈**
   → 해결: 먼저 데이터·손실·학습률 점검

---

## 학습이 안 되는 5가지 원인: 디버깅 루틴

1. 손실이 줄어드는가? → 아니면 학습률/손실 문제
2. 출력 분포가 한쪽으로 쏠리는가? → 불균형/임계값
3. Train은 잘 되고 Val만 안 되는가? → 과적합
4. 입력 스케일이 제각각인가? → 스케일링
5. 베이스라인보다 나쁜가? → 문제 정의 재점검

---

## 미니 Q&A

1. **ANN만으로도 충분한 문제는?**
   표 형태 데이터 대부분.

2. **은닉층은 몇 개가 적당한가?**
   작게 시작(1~2개) → 필요 시 증가.

3. **ReLU가 항상 정답인가?**
   은닉층 기준으로는 거의 그렇다.

4. **역전파를 수식으로 알아야 하나?**
   아니다. 계산 그래프 흐름 이해가 핵심.

5. **ANN 이후 바로 CNN/RNN 가도 되나?**
   ANN 학습 흐름이 머리에 잡혔다면 가능.

---

## 다음 편 예고

다음 편에서는 **ANN을 실제 코드로 구현**한다.
이론이 “학습이 된다”는 감각이었다면, 다음은 **왜 안 되는지 디버깅하는 법**이다.
고객 이탈 예제를 통해 ANN 구현의 실전 기준선을 만든다.

---

- 참고: [딥러닝의 모든 것 with Python, Tensorflow, Pytorch
](https://www.udemy.com/course/best-artificial-neural-networks/)
