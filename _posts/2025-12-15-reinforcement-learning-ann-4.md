---
title: "DCQN: CNN으로 ‘보는’ 강화학습 만들기 (Atari·팩맨 계열)"
description: "이미지 상태를 위한 DCQN 전처리, CNN 구조, 디버깅 체크리스트를 정리한 가이드"
categories: ["🔁 Reinforcement Learning & ANN"]
tags: [DCQN, CNN, Atari, ReinforcementLearning]
image: /assets/posts/2025-12-15-reinforcement-learning-ann/image.jpg
date: 2025-12-15 21:43:00 +09:00
last_modified_at: 2025-12-15 21:43:00 +09:00
---

## 이 글의 목적

**핵심 요약**

* DCQN(Deep Convolutional Q-Network)은 **“이미지를 상태로 쓰는 DQN”**이다.
* 난이도는 알고리즘이 아니라 **상태 표현(vision)** 에서 급상승한다.
* 핵심은 CNN, 전처리, 시간 정보 처리, 그리고 연산 효율이다.
* 이 글은 Atari/팩맨류 환경을 **재현·확장 가능한 템플릿**으로 정리한다.

**예시**

* 팩맨 화면을 그대로 입력으로 받아 행동을 선택하는 에이전트
* 보상은 늦게 오지만, 화면 변화로 정책을 학습

---

## 1. 이미지 상태에서 무엇이 어려워지는가

### 정의

* DCQN은 상태 `s`가 **저차원 벡터가 아니라 이미지(frame)** 인 강화학습이다.

**핵심 요약**

* 이미지 상태는 정보가 많지만, **불필요한 정보도 압도적으로 많다**.
* 단순히 “보인다” ≠ “학습된다”.
* 주요 난점은 ① 차원 폭발 ② 노이즈 ③ 시간 정보 손실이다.

### 주요 어려움 정리

| 문제      | 설명                         | 결과           |
| --------- | ---------------------------- | -------------- |
| 차원 폭발 | 210×160×3 RGB                | 학습 불안정    |
| 노이즈    | 배경/점수/UI                 | 특징 추출 방해 |
| 시간 정보 | 단일 프레임은 속도/방향 모름 | 정책 오판      |

---

## 2. 전처리: 이미지를 ‘상태’로 만들기

**핵심 요약**

* 전처리는 성능의 절반 이상을 좌우한다.
* 목적은 **정보 압축 + 불변성 확보 + 시간 정보 보존**이다.
* Atari 계열의 표준 전처리 파이프라인은 이유가 있다.

### 전처리 단계별 목적

| 단계        | 입력        | 출력    | 이유                 |
| ----------- | ----------- | ------- | -------------------- |
| Grayscale   | RGB         | 1채널   | 색상 제거, 차원 축소 |
| Resize      | 원본 해상도 | 84×84   | 연산량 감소          |
| Normalize   | 픽셀 값     | [0,1]   | 학습 안정            |
| Frame Stack | 단일 프레임 | 4프레임 | 시간 정보 확보       |

**직관**

* CNN은 “형태”를 본다.
* 색·해상도는 대부분 **형태 인식에 불필요**하다.

---

## 3. CNN은 무엇을 뽑고, Q-learning과 어떻게 연결되는가

### 정의

* CNN은 이미지에서 **공간적 패턴(feature)** 을 추출하는 모듈이다.

**핵심 요약**

* CNN은 “적의 위치”, “장애물 패턴” 같은 **고수준 특징**을 만든다.
* 이 특징 벡터가 DQN의 입력이 된다.
* Q-learning 로직은 바뀌지 않고, **상태 표현만 강화**된다.

### 연결 구조 (텍스트 다이어그램)

```text
Image Frames
   ↓ (CNN)
Feature Vector
   ↓ (FC layers)
Q(s, a1), Q(s, a2), ...
```

**중요 포인트**

* CNN은 상태 인코더
* Q-network의 **앞단(front-end)** 역할

---

## 4. CNN 아키텍처 예시 (Atari 표준)

**핵심 요약**

* 깊은 네트워크보다 **검증된 얕은 구조**가 낫다.
* 출력 차원을 추적하지 않으면 구현 단계에서 오류가 난다.

### 예시 아키텍처

| Layer   | Kernel / Stride | Output Shape |     |     |
| ------- | --------------- | ------------ | --- | --- |
| Input   | 4×84×84         | 4×84×84      |     |     |
| Conv1   | 32, 8×8 / 4     | 32×20×20     |     |     |
| Conv2   | 64, 4×4 / 2     | 64×9×9       |     |     |
| Conv3   | 64, 3×3 / 1     | 64×7×7       |     |     |
| Flatten | —               | 3136         |     |     |
| FC      | 512             | 512          |     |     |
| Output  | actions         |              | A   |     |

**구현 포인트**

* Conv 이후 ReLU 필수
* Output에는 activation 없음

---

## 5. GPU 최적화 포인트 (실무 기준)

**핵심 요약**

* DCQN은 **연산량보다 메모리 병목**이 먼저 온다.
* GPU 사용 여부보다 **데이터 이동 방식**이 중요하다.

### 주요 최적화 포인트

| 항목          | 체크                       |
| ------------- | -------------------------- |
| Batch size    | GPU 메모리 허용 범위까지 ↑ |
| Tensor device | CPU↔GPU 이동 최소화        |
| Replay buffer | CPU 저장, batch만 GPU      |
| dtype         | float32 고정               |

**실전 팁**

* `.to(device)` 호출 위치 하나로 속도 2배 차이 난다.

---

## 6. 시각화: 학습을 “보는” 이유

**핵심 요약**

* DCQN은 숫자만 보면 실패 원인을 알기 어렵다.
* 시각화는 디버깅 도구다.

### 필수 시각화 3종

| 시각화         | 의미             |
| -------------- | ---------------- |
| Reward curve   | 학습 진행 여부   |
| Moving average | 노이즈 제거 추세 |
| 플레이 영상    | 정책의 질적 평가 |

**예시**

* Reward 상승하지만 영상은 엉망 → 보상 설계 문제
* Reward 정체 + 영상 반복 → 탐험 문제

---

## 7. 성능이 안 나올 때 점검 순서 (중요)

**핵심 요약**

* 네트워크를 키우기 전에 **상태·전처리·보상**을 먼저 의심한다.
* DCQN 실패의 대부분은 알고리즘이 아니라 **입력 문제**다.

### 디버깅 체크리스트 (순서 중요)

1. 전처리 출력이 정상 이미지인가
2. Frame stack이 실제로 시간 정보를 담는가
3. Reward가 충분히 자주 발생하는가
4. ε 감소가 너무 빠르지 않은가
5. Replay buffer가 너무 작지 않은가
6. Target network 업데이트가 정상인가
7. GPU/CPU 병목이 없는가

---

## 흔히 하는 오해 3가지와 교정

**오해 1**: “CNN을 깊게 하면 성능이 오른다”
→ **교정**: 대부분 과적합·불안정만 증가한다.

**오해 2**: “이미지니까 보상이 늦어도 된다”
→ **교정**: Sparse reward는 DCQN의 최대 적이다.

**오해 3**: “Reward만 보면 학습 여부를 안다”
→ **교정**: 영상 없이는 정책 품질을 판단할 수 없다.

---

## 핵심 체크리스트 (재학습용)

* [ ] 이미지 상태의 3대 난점을 설명할 수 있는가
* [ ] 각 전처리 단계의 목적을 말로 설명할 수 있는가
* [ ] CNN 출력이 Q-network 입력으로 어떻게 연결되는지 아는가
* [ ] Atari 표준 CNN 구조를 재현할 수 있는가
* [ ] GPU 병목 포인트를 인지하고 있는가
* [ ] Reward curve와 영상의 불일치를 해석할 수 있는가
* [ ] 성능 저하 시 전처리부터 점검하는가

---

## 한 줄 요약

**DCQN의 본질은 강화학습이 아니라, “이미지를 상태로 바꾸는 표현 설계 문제”다.**

---

- 참고: [ AI 만들기 2025: 강화학습과 인공신경망 완전정복, Agentic AI, Gen AI, RL
](https://www.udemy.com/course/best-ai-17-hours/)
