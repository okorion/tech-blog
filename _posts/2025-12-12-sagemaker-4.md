---
title: "XGBoost와 하이퍼파라미터 튜닝, 실무 기준으로 이해하기"
description: "XGBoost 핵심 개념과 SageMaker 튜닝 패턴을 정리하고, 실무에서 바로 쓸 탐색 범위와 Objective 설정 기준을 요약"
categories: ["🤖 AWS Sagemaker"]
tags: [XGBoost, 하이퍼파라미터, SageMaker, 튜닝]
image: /assets/posts/2025-12-12-sagemaker/image.png
date: 2025-12-12 17:58:00 +09:00
last_modified_at: 2025-12-12 17:58:00 +09:00
---

# XGBoost와 하이퍼파라미터 튜닝, 실무 기준으로 이해하기

XGBoost는 **트리 기반 모델의 해석력 + 부스팅의 강력한 성능 + 정규화 기반의 안정성**을 모두 갖춘 실전형 알고리즘이다.
선형 회귀가 “관계가 직선에 가까운 데이터”에서 강력하다면, XGBoost는 **비선형 패턴·상호작용·잡음 속에서도 일관된 성능을 내는 모델**로서 구조적 우위를 가진다.

### 이번 글 핵심 요약 4~6줄

* XGBoost는 **비선형성·피처 상호작용·이상치가 많은 데이터**에서 선형 모델보다 강하다.
* 우선 튜닝해야 할 파라미터는 **max_depth, learning_rate(eta), num_round**이다.
* 다음으로 **subsample, colsample_bytree**로 과적합 방지와 일반화 성능을 조절한다.
* SageMaker에서는 **Estimator 정의 → Tuning Job 생성 → Best Model 자동 저장** 흐름으로 재사용한다.
* objective metric을 정확히 설정하는 것이 튜닝 품질의 핵심이다.
* search space(탐색 범위)는 “너무 좁으면 개선 없음, 너무 넓으면 시간/비용 낭비”가 된다.

---

## XGBoost 핵심 개념 직관 요약

XGBoost는 본질적으로 다음 세 축으로 설명된다.

1. **결정 트리(Decision Tree)**

   * 데이터 분할 규칙을 통해 예측하는 구조
   * 단일 트리는 고분산(variance)이 크다 → 과적합 위험

2. **앙상블(Ensemble)**

   * 여러 트리를 조합해 안정성 확보
   * 랜덤 포레스트는 “여러 트리를 병렬로 학습”한다면,
     **XGBoost는 여러 트리를 순차적으로 개선하며 학습**

3. **부스팅(Boosting)**

   * 이전 트리가 틀린 부분을 다음 트리가 집중해서 보완
   * “누적 학습으로 점점 더 나아지는” 구조

### 편향/분산 및 정규화와의 관계

* **max_depth**를 깊게 하면 → 편향 ↓ / 분산 ↑ → 과적합 위험
* **learning_rate(eta)**를 낮추면 → 편향 ↑ / 분산 ↓ → 더 많은 트리 필요
* **subsample / colsample_bytree**는 랜덤성 부여 → 분산 감소 → 일반화 향상
* **lambda, alpha(L1/L2)**는 가중치 크기 조절 → 지나친 트리 복잡도 억제

### 실무에서 XGBoost가 자주 선택되는 이유

* 전처리 부담이 적다(정규화 불필요, 범주 인코딩만 하면 됨)
* 피처 간 상호작용을 자동으로 학습
* 이상치(outlier)에 강하다
* 대규모 데이터에서 뛰어난 속도
* 하이퍼파라미터 튜닝으로 성능 확장성 높음
* 회귀·분류 어디든 안정적인 baseline이 된다

---

## SageMaker에서 XGBoost 모델 학습 패턴

### 1) Estimator 기본 패턴

```python
from sagemaker.xgboost import XGBoost

xgb_estimator = XGBoost(
    entry_point=None,             # 기본 XGBoost 컨테이너 사용
    framework_version="1.2-1",
    role=role,
    instance_type="ml.m5.xlarge",
    instance_count=1,
    output_path=s3_output_path,
    hyperparameters={
        "objective": "reg:squarederror",
        "max_depth": 5,
        "eta": 0.1,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "num_round": 200
    }
)
```

### 2) 데이터 학습

```python
xgb_estimator.fit({"train": train_s3_uri, "validation": val_s3_uri})
```

### 주요 하이퍼파라미터 요약표

| 파라미터               | 의미           | 모델 영향                          |
| ---------------------- | -------------- | ---------------------------------- |
| **max_depth**          | 트리 최대 깊이 | 클수록 복잡 → 과적합 위험 증가     |
| **eta(learning_rate)** | 학습률         | 낮을수록 안정적, 트리 수 증가 필요 |
| **num_round**          | 트리 개수      | 모델 복잡도 결정                   |
| **subsample**          | 샘플 하위비율  | 과적합 방지, 일반화 향상           |
| **colsample_bytree**   | 피처 하위비율  | 피처 랜덤성 증가로 robust          |
| **lambda (L2)**        | L2 규제        | 가중치 안정화                      |
| **alpha (L1)**         | L1 규제        | 피처 선택 효과                     |

### 이 섹션에서 기억해야 할 코드 패턴

* `XGBoost(..., hyperparameters={...})`
* `.fit({"train": ..., "validation": ...})`
* 핵심 파라미터는 XGBoost 공식 알고리즘과 이름이 거의 동일

---

## Hyperparameter Tuning Job 구조 이해

### SageMaker 튜닝의 핵심 구성요소

1. **objective metric**

   * ex) `"validation:rmse"`
   * 튜너는 이 값을 최소화/최대화하도록 탐색한다.

2. **search space**

   * ex) max_depth: 3~10
   * 숫자가 너무 크면 비용 증가, 너무 작으면 개선 없음

3. **search strategy**

   * Random search(강의 기본)
   * Bayesian Optimization(더 효율적, 실전 추천)

### 튜닝 Job 예시 코드

```python
from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter

tuner = HyperparameterTuner(
    estimator=xgb_estimator,
    objective_metric_name="validation:rmse",
    objective_type="Minimize",
    max_jobs=20,
    max_parallel_jobs=3,
    hyperparameter_ranges={
        "max_depth": IntegerParameter(3, 10),
        "eta": ContinuousParameter(0.01, 0.3),
        "subsample": ContinuousParameter(0.5, 1.0),
        "colsample_bytree": ContinuousParameter(0.5, 1.0)
    }
)

tuner.fit({"train": train_s3_uri, "validation": val_s3_uri})
```

### search range 설정 기준

* **max_depth**: 문제 복잡성에 따라 3~10 범위가 안정적 시작점
* **eta**: 0.01~0.3 정도로 탐색 → 학습률이 낮으면 트리 수 증가 필요
* **subsample/colsample**: 0.5~1.0 범위로 탐색 → 일반화 성능 결정
* feature 수가 많으면 colsample을 더 낮게 설정 가능

### 이 섹션에서 기억해야 할 패턴

* `HyperparameterTuner(estimator, objective_metric, ranges)`
* `.fit()`만 호출하면 튜닝 + 베스트 모델 저장까지 자동 실행

---

## 프로젝트 #3: 가게 매출 예측 워크플로우 요약

트리 기반 회귀 문제를 수행하는 가장 전형적인 구조다.
**이 흐름은 어떤 XGBoost 회귀 문제에도 그대로 재사용 가능**하다.

### 1) 데이터 병합/전처리

* 여러 테이블 조인 시 key 누락/중복 주의
* 날짜가 포함되면 `월/요일/휴일 여부` 등으로 분해
* 범주형은 원핫 인코딩 또는 숫자 인코딩

### 2) 로컬 XGBoost baseline

* Python XGBoost로 baseline 성능 파악
* 데이터 누락/이상치를 빨리 확인하는 단계

### 3) SageMaker XGBoost 학습

* S3에 CSV 업로드
* Estimator 정의 후 `.fit()` 호출하면 자동으로 Training Job 실행

### 4) Hyperparameter Tuning

* RMSE 기준으로 best model 탐색
* 튜닝 후 성능이 baseline 대비 의미 있게 개선됨

  * (강의 기준: RMSE가 확연히 감소하는 경향)

### 5) 배포 및 추론

* `.deploy()`로 endpoint 생성
* 실무에서는 비용 절감을 위해 배포 인스턴스 최소화

---

### 이 섹션에서 기억해야 할 패턴

* “로컬 baseline → SageMaker Estimator → 튜닝 → 재학습 → 배포”
* 데이터 구조만 바뀌어도 전체 흐름은 동일

---

## 하이퍼파라미터 튜닝 전략: 실무 의사결정 가이드

XGBoost 튜닝은 “모든 파라미터를 무작정 탐색”하는 것이 아니라,
**모델 성능에 효과가 가장 큰 순서로 좁혀가야 한다.**

### 1단계: 모델 구조 결정(가장 영향력이 큼)

* **max_depth**

  * 깊을수록 복잡 → 과적합 위험
  * 먼저 조정해야 전체 모델의 방향이 결정됨
* **learning_rate(eta)**

  * 너무 크면 불안정, 너무 작으면 수렴 느림
* **num_round**

  * 트리 개수 → 학습 지속 시간 결정

### 2단계: 일반화 성능 튜닝

* **subsample**

  * 데이터를 부분 샘플링하여 편향·분산 균형
* **colsample_bytree**

  * 피처 랜덤 선택으로 과적합 방지

### 3단계: 정규화 파라미터

* **lambda(L2), alpha(L1)**

  * 과적합이 여전히 발생할 때 적용
  * 피처가 많고 잡음이 많을 때 유용

---

### 예산 기반 전략

#### 빠른 Rough Search(5~10분 예산)

* max_depth: 3~8
* eta: 0.05~0.3
* subsample: 0.7~1.0
* num_round 고정(100~200)
  → “대략 어떤 구조가 성능이 나오는지” 확인

#### 정교한 Fine Tuning(1~3시간 예산)

* best region 중심으로 eta 0.01 단위 조정
* max_depth 1씩 좁혀 탐색
* subsample/colsample을 0.1 단위로 좁게 탐색
  → 모델이 실제 서비스 수준으로 안정화됨

---

## 튜닝 실패 사례 또는 주의해야 할 점

* search range가 너무 좁아서 개선이 거의 안 발생
* learning_rate를 너무 낮게 설정해 학습이 수렴하지 않음
* max_depth를 너무 크게 설정하여 과적합 + 느린 학습
* objective metric을 validation이 아닌 train 값으로 설정
* 튜닝 job의 `max_parallel_jobs`를 과도하게 늘려 비용 폭발

---

- 참고: [초보자를 위한 AWS SageMaker 실습 | 6개 프로젝트 구축하기
](https://www.udemy.com/course/best-aws-sagemaker/learn/lecture/29630912?start=15#overview)
