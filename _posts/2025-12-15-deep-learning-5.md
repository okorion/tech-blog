---
title: "RNN/LSTM 직관: hidden state, 시퀀스, vanishing gradient를 ‘왜’로부터 이해하기"
description: "hidden state와 시퀀스 의존성, vanishing gradient, LSTM 게이트 직관으로 RNN을 이해"
categories: ["🧬 Deep Learning"]
tags: [RNN, LSTM, VanishingGradient, SequenceModeling]
image: /assets/posts/2025-12-15-deep-learning/image.jpg
date: 2025-12-15 21:54:00 +09:00
last_modified_at: 2025-12-15 21:54:00 +09:00
---

# RNN/LSTM 직관: hidden state, 시퀀스, vanishing gradient를 ‘왜’로부터 이해하기

---

## TL;DR

* RNN은 **“이전 정보가 현재 판단에 영향을 주는 문제”**를 풀기 위한 구조다.
* 핵심 개념은 뉴런이 아니라 **hidden state(기억)** 다.
* 시계열 문제의 최대 리스크는 **데이터 누수(미래 정보 유입)** 이다.
* 기본 RNN은 **vanishing gradient** 때문에 장기 기억에 약하다.
* LSTM은 “기억을 선택적으로 저장/삭제”하는 **게이트 구조**로 이 문제를 완화한다.
* 주가 예측에서 성능보다 중요한 것은 **윈도잉·스케일링·평가 방식의 정합성**이다.
* RNN/LSTM은 여전히 유용하지만, **병렬화·장기 의존성**에서는 한계가 명확하다.

---

## 전체 지도: RNN/LSTM은 딥러닝에서 어디에 위치하는가

**한 문장 요약:** RNN/LSTM은 ANN의 확장판으로, “시간 축”을 모델 구조에 직접 포함시킨 모델이다.

딥러닝 모델을 “입력 구조” 기준으로 보면 다음과 같다.

| 데이터 구조        | 대표 모델   | 핵심 가정       |
| ------------------ | ----------- | --------------- |
| 표(tabular)        | ANN         | 샘플 독립       |
| 이미지             | CNN         | 공간적 지역성   |
| 시계열/문장        | RNN/LSTM    | **순서 의존성** |
| 장문/대규모 시퀀스 | Transformer | 전역 의존성     |

RNN은

> “지금의 판단은 과거의 맥락을 알아야 한다”
> 는 가정을 구조로 구현한 첫 표준 모델이다.

---

## 핵심 개념: 왜 RNN이 필요하고, 무엇이 다른가

### 1) 시퀀스(sequence) 문제의 본질

**한 문장 요약:** 시계열 문제는 “샘플이 서로 독립이 아니다”.

* 주가, 센서, 로그, 문장 모두

  * 현재 값은 과거 값의 영향을 받는다.
* ANN/CNN처럼 “각 샘플을 독립”으로 보면

  * 시간 정보가 사라진다.

**미니 사례:**
어제·그제 주가를 무시하고 오늘 종가만 보고 내일을 예측하는 모델은 현실과 맞지 않는다.

---

### 2) Hidden State란 무엇인가

**한 문장 요약:** hidden state는 “지금까지의 정보를 요약한 내부 기억”이다.

RNN의 핵심 아이디어는 단순하다.

```
입력 x_t + 이전 상태 h_{t-1}
 → 현재 상태 h_t
```

* h_t는:

  * 지금 입력(x_t)
  * 과거 요약(h_{t-1})
    를 합친 결과다.

**비유:**
hidden state는 “회의록 요약본”이다.
모든 발언을 다 기억하지 않고, 중요한 흐름만 다음 회의로 가져간다.

---

### 3) RNN이 학습하기 어려운 이유: Vanishing Gradient

**한 문장 요약:** 기본 RNN은 “오래된 정보일수록 학습 신호가 사라진다”.

* 역전파 시:

  * 시간 축을 따라 기울기가 반복 곱셈
  * 1보다 작은 값이 계속 곱해지면 → 0으로 수렴
* 결과:

  * 최근 정보만 기억
  * 먼 과거 정보는 학습 불가

**미니 사례:**
주가에서

* “5일 전 패턴”은 반영
* “3개월 전 추세”는 무시
  → 단기 예측만 가능해진다.

---

## LSTM: 왜 등장했고, 무엇이 다른가

### 1) LSTM의 핵심 아이디어

**한 문장 요약:** LSTM은 “기억을 그냥 전달하지 않고, 선택적으로 관리”한다.

* RNN: 기억을 그냥 넘김
* LSTM: 기억을

  * 저장할지
  * 유지할지
  * 버릴지
    결정

---

### 2) LSTM Gate 구조 (역할 분해)

**한 문장 요약:** LSTM은 3개의 게이트로 기억을 제어한다.

| 게이트      | 역할                | 직관                       |
| ----------- | ------------------- | -------------------------- |
| Forget Gate | 이전 기억 삭제 여부 | “이 정보, 이제 필요 없어?” |
| Input Gate  | 새 정보 저장 여부   | “이건 기억할 만해?”        |
| Output Gate | 외부로 출력할 정보  | “지금 꺼낼 정보는?”        |

#### 블록 다이어그램 관점(개념)

```
이전 기억(Cell State)
 ├─ Forget Gate → 버릴 것 제거
 ├─ Input Gate  → 새 정보 추가
 └─ Output Gate → 현재 출력 결정
```

**핵심 차이:**

* hidden state와 **cell state(장기 기억)** 를 분리
* 기울기가 cell state를 통해 **직접 전달** → vanishing 완화

---

### 3) LSTM 변형들

**한 문장 요약:** 변형은 많지만, 기본 LSTM 이해가 우선이다.

* Peephole LSTM
* GRU (Gate 수를 줄인 단순화 버전)

실무에서는:

* **LSTM 또는 GRU 중 하나**만 제대로 써도 충분하다.

---

## 작동 원리: 입력→출력 흐름, 학습 흐름

### 1) 입력→출력(Forward)

**한 문장 요약:** RNN/LSTM은 “시퀀스를 한 타임스텝씩 처리”한다.

```
(x1) → h1
(x2) → h2
(x3) → h3
...
```

* 주가 예측:

  * 입력: 과거 N일 가격 벡터
  * 출력: 다음 날 가격(또는 수익률)

---

### 2) 학습(Training)

**한 문장 요약:** 학습은 “시간을 펼친 네트워크”에서 이루어진다.

* Backpropagation Through Time (BPTT)
* LSTM은:

  * 장기 기억 경로를 통해
  * 기울기가 덜 사라지게 설계됨

---

## 구현 체크리스트: 시계열에서 반드시 지켜야 할 것

### 데이터 (⚠️ 가장 중요)

**한 문장 요약:** 시계열 모델 실패의 80%는 데이터 누수다.

#### 데이터 누수 3대 유형 (구체 경고)

1. **미래 정보 포함**

* 예: 이동평균을 전체 데이터로 계산
* 해결: 항상 과거 기준으로 계산

2. **스케일링 누수**

* 전체 데이터로 scaler.fit()
* 해결: train 데이터로만 fit, 나머지는 transform

3. **윈도잉 오류**

* 입력 윈도우에 미래 시점 포함
* 해결: 인덱스 기준 명확화

---

### 모델

* [ ] 기본 RNN vs LSTM 비교
* [ ] 시퀀스 길이(window size) 명시
* [ ] 출력 구조(one-to-one / many-to-one) 정의

### 학습

* [ ] 학습률 낮게 시작
* [ ] gradient clipping 사용
* [ ] EarlyStopping 필수

### 평가

* [ ] 랜덤 셔플 금지
* [ ] 롤링/워크포워드 평가

### 디버깅

* [ ] 예측이 “평균값”으로 수렴하는지 확인
* [ ] train/val 분포 차이 점검

---

## 실전 적용: 주가 예측에서 RNN/LSTM을 쓸 때

**한 문장 요약:** 주가 예측은 “모델보다 검증 설계”의 문제다.

### 실무 포인트

* 목표:

  * 가격 자체보다 **방향/수익률** 예측이 현실적
* 리스크:

  * 과거 패턴의 재현 가능성 낮음
  * 작은 성능 차이도 과적합일 수 있음
* 지표:

  * MSE 단독 금지
  * 방향 정확도, 누적 수익률 보조 지표 필요

**미니 사례:**
Validation 성능이 좋은데 실전 수익이 나쁘다
→ 대부분 **데이터 누수 또는 평가 설계 오류**다.

---

## RNN/LSTM의 한계(솔직히)

**한 문장 요약:** RNN은 만능이 아니다.

| 한계        | 이유               |
| ----------- | ------------------ |
| 장기 의존성 | LSTM도 한계 존재   |
| 병렬화      | 시퀀스 순차 처리   |
| 학습 속도   | 긴 시퀀스에서 느림 |

→ 이 한계를 극복하려고 **Transformer**가 등장했다.
하지만 RNN/LSTM을 이해하지 못하면 Transformer도 “블랙박스”가 된다.

---

## 흔한 함정 TOP 7 + 해결책

1. **랜덤 셔플로 train/val 분리**
   → 해결: 시간 순 분리 고정

2. **전체 데이터로 스케일링**
   → 해결: train 기준 fit

3. **윈도우에 미래 정보 포함**
   → 해결: 인덱스 수동 검증

4. **MSE만 보고 성능 판단**
   → 해결: 방향/리스크 지표 추가

5. **LSTM이면 다 해결된다고 착각**
   → 해결: 데이터/평가 우선 점검

6. **시퀀스 길이 과도하게 증가**
   → 해결: 짧은 window부터 실험

7. **과거 성능을 미래 성능으로 착각**
   → 해결: 롤링 검증 필수

---

## 미니 Q&A

1. **RNN과 LSTM 중 무엇부터?**
   LSTM부터 써라. 기본 RNN은 교육용이다.

2. **시퀀스 길이는 어떻게 정하나?**
   도메인 지식 + 실험. 길수록 좋지 않다.

3. **주가 예측이 정말 가능한가?**
   제한적이다. 예측보다 리스크 관리가 목적이다.

4. **CNN으로 시계열 처리해도 되나?**
   가능하다(1D CNN). 짧은 패턴엔 유리하다.

5. **Transformer로 바로 가도 되나?**
   가능하지만, RNN 감각이 있으면 이해 속도가 다르다.

---

## 다음 편 예고

다음 편에서는 **RNN/LSTM 구현 실전**으로 들어간다.
핵심은 모델 코드가 아니라 **윈도잉·누수 방지·롤링 평가**다.
이번 편에서 잡은 직관이 구현 단계에서 바로 검증된다.

---

- 참고: [딥러닝의 모든 것 with Python, Tensorflow, Pytorch
](https://www.udemy.com/course/best-artificial-neural-networks/)
