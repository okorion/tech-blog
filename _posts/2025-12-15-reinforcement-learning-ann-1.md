---
title: "강화학습 한 장 요약: MDP, Policy, Value/Q, On·Off-policy, 그리고 알고리즘 계보"
description: "MDP·Policy·Value/Q·On/Off-policy 핵심과 알고리즘 계보를 한 장으로 정리한 RL 개요"
categories: ["🔁 Reinforcement Learning & ANN"]
tags: [ReinforcementLearning, MDP, Policy, QLearning]
image: /assets/posts/2025-12-15-reinforcement-learning-ann/image.jpg
date: 2025-12-15 21:40:00 +09:00
last_modified_at: 2025-12-15 21:40:00 +09:00
---

## 이 글의 목적

**핵심 요약**

* 강화학습(RL)을 구성하는 **최소 단위 개념**을 한 장으로 정리한다.
* 알고리즘 암기보다 **왜 이런 구조가 필요한지**에 초점을 둔다.
* 이후 DQN, A3C, PPO/SAC를 볼 때 **기준 좌표계**로 재사용 가능하게 만든다.
* 실무에서 “문제에 맞는 알고리즘 선택”이 가능해지는 것이 목표다.

**예시**

* 게임 환경을 봤을 때: 상태/행동/보상을 즉시 정의
* 로보틱스 문제에서: 연속 행동 여부로 PPO/SAC 판단
* 추천 문제에서: Off-policy 필요성 즉시 판단

---

## 1. MDP: 강화학습 문제를 정의하는 최소 단위

**정의**

* 강화학습 문제는 **MDP(Markov Decision Process)** 로 표현된다.
* MDP는 환경과 에이전트의 상호작용을 **수학적으로 고정**하는 틀이다.

**핵심 요약**

* MDP는 “학습 알고리즘”이 아니라 **문제 정의 방식**이다.
* MDP가 명확하지 않으면 어떤 알고리즘을 써도 실패한다.
* 실무에서 RL 실패의 70%는 **MDP 정의 오류**다.

### MDP 구성 요소와 실전 의미

| 요소           | 의미      | 실전에서의 해석                 |
| -------------- | --------- | ------------------------------- |
| S (State)      | 상태      | 관측값 벡터, 이미지, 센서 값    |
| A (Action)     | 행동      | 버튼 입력, 토크 값, 추천 아이템 |
| P (Transition) | 전이 확률 | 환경의 동역학(보통 모름)        |
| R (Reward)     | 보상      | KPI, 점수, 성공/실패 신호       |
| γ (Discount)   | 할인율    | 미래 보상의 중요도              |

**예시**

* 자율주행

  * S: 카메라 이미지 + 속도
  * A: 조향각, 가속
  * R: 차선 유지, 충돌 패널티
* 게임

  * S: 화면 프레임
  * A: 조작 입력
  * R: 점수 변화

---

## 2. Policy, V(s), Q(s,a): 역할과 관계

**정의**

* **Policy(π)**: 상태에서 어떤 행동을 할지 정하는 규칙
* **V(s)**: 해당 상태가 얼마나 좋은지
* **Q(s,a)**: 특정 상태에서 특정 행동이 얼마나 좋은지

**핵심 요약**

* Policy는 “행동 규칙”, Value는 “평가 함수”다.
* Q는 **행동 선택까지 포함한 평가**라서 실제 제어에 직접 쓰인다.
* 강화학습의 대부분 알고리즘은 이 셋 중 하나를 직접/간접적으로 학습한다.

### 텍스트 다이어그램으로 보는 관계

```text
State (s)
  ├─ V(s): 이 상태 자체는 얼마나 좋은가?
  └─ Q(s,a): 이 상태에서 이 행동을 하면 얼마나 좋은가?

Policy π(a|s)
  └─ Q(s,a) 또는 V(s)를 참고해 행동을 선택
```

**직관**

* V(s)만 있으면 “어디가 좋은지”는 알지만 **무엇을 할지**는 모른다.
* Q(s,a)가 있어야 행동 간 비교가 가능하다.

---

## 3. 왜 V에서 Q로 넘어갔는가?

**핵심 요약**

* 초기 강화학습은 V(s) 중심이었지만, **행동 선택 문제**가 남았다.
* Q(s,a)는 평가와 선택을 동시에 해결한다.
* 이 전환이 **Q-learning → DQN**으로 이어진다.

**왜 필요했나**

* 실제 문제는 항상 “다음에 뭘 할지”를 요구한다.
* V(s)만으로는 정책을 명확히 정의하기 어렵다.

**예시**

* 체스에서 “현재 판이 좋다”는 것만 알면 부족
  → 어떤 수를 둬야 하는지가 핵심

---

## 4. 강화학습 알고리즘 계보 (한 눈에)

**핵심 요약**

* 알고리즘은 무작위로 늘어난 게 아니라 **문제 해결 과정의 결과**다.
* 핵심 분기점은 두 가지:

  1. 무엇을 직접 학습하는가? (Value vs Policy)
  2. 데이터를 어떻게 쓰는가? (On vs Off)

### 계보 흐름

```text
Value-based
  Q-learning → DQN

Policy-based
  Policy Gradient

Actor-Critic
  Actor-Critic → A3C → PPO / SAC
```

---

## 5. Value / Policy / Actor-Critic 비교

### 비교표 ①: 학습 대상 기준

| 구분         | 직접 학습    | 장점              | 단점             |
| ------------ | ------------ | ----------------- | ---------------- |
| Value-based  | V(s), Q(s,a) | 안정적, 이해 쉬움 | 연속 행동 어려움 |
| Policy-based | π(a \| s)    | 직접 제어 가능    | 분산 큼          |
| Actor-Critic | π + V/Q      | 안정성과 유연성   | 구조 복잡        |


**구현 포인트**

* Value-based: Q-table / Q-network
* Policy-based: log-prob 기반 업데이트
* Actor-Critic: 두 네트워크 동시 관리

---

## 6. On-policy vs Off-policy

**정의**

* **On-policy**: 현재 정책으로 모은 데이터만 사용
* **Off-policy**: 과거/다른 정책의 데이터도 사용

### 비교표 ②: 데이터 사용 기준

| 구분       | 대표 알고리즘 | 특징           | 실전 의미   |
| ---------- | ------------- | -------------- | ----------- |
| On-policy  | PPO           | 안정적         | 데이터 낭비 |
| Off-policy | DQN, SAC      | 샘플 효율 높음 | 불안정 가능 |

**직관**

* On-policy: “지금의 나”로만 학습
* Off-policy: “과거의 나 기록”도 재사용

---

## 7. 실전 문제 유형별 알고리즘 매핑

**핵심 요약**

* 알고리즘 선택은 취향이 아니라 **문제 구조**의 결과다.

| 문제 유형       | 특징               | 적합 알고리즘   |
| --------------- | ------------------ | --------------- |
| 게임(이산 행동) | 상태 큼, 행동 적음 | DQN / DCQN      |
| 로보틱스        | 연속 행동          | PPO / SAC       |
| 추천/의사결정   | 로그 데이터        | Off-policy 계열 |

**성공 기준**

* 게임: 평균 reward 상승
* 로보틱스: 안정적 제어
* 추천: 장기 KPI 개선

---

## 8. 용어 정리 (필수)

| 용어      | 의미                  |
| --------- | --------------------- |
| Episode   | 시작~종료까지 한 시도 |
| Step      | 한 번의 상태 전이     |
| Return    | 누적 보상             |
| Terminal  | 종료 상태             |
| Advantage | 행동의 상대적 이점    |
| Entropy   | 정책의 불확실성       |

---

## 흔히 하는 오해 3가지와 교정

**오해 1**: “MDP는 이론용이다”
→ **교정**: 실무 RL 문제 정의의 90%가 MDP 설계다.

**오해 2**: “DQN은 구식이다”
→ **교정**: 개념적 기준점으로 여전히 필수다.

**오해 3**: “PPO/SAC는 그냥 더 좋은 알고리즘이다”
→ **교정**: 문제 조건이 맞을 때만 강력하다.

---

## 핵심 체크리스트 (재학습용)

* [ ] 상태/행동/보상을 즉시 정의할 수 있는가
* [ ] V와 Q의 차이를 말로 설명할 수 있는가
* [ ] Q에서 Policy로 넘어간 이유를 이해했는가
* [ ] On/Off-policy 차이를 데이터 관점에서 설명 가능한가
* [ ] Actor와 Critic 역할을 구분하는가
* [ ] 문제 유형에 맞는 알고리즘을 고를 수 있는가
* [ ] 실패 원인을 알고리즘이 아닌 MDP에서 먼저 찾는가

---

## 한 줄 요약

**강화학습은 알고리즘 공부가 아니라, MDP를 정확히 정의하고 그에 맞는 학습 대상을 고르는 문제다.**

---

- 참고: [ AI 만들기 2025: 강화학습과 인공신경망 완전정복, Agentic AI, Gen AI, RL
](https://www.udemy.com/course/best-ai-17-hours/)
