---
title: "NLP 분류 문제의 최소 구조"
description: "텍스트 분류를 최소 구조로 환원해 표현·모델 선택 기준을 정리한 NLP 기준서"
categories: ["🧠 Machine Learning Projects"]
tags: [NLP, 텍스트분류, CountVectorizer, NaiveBayes]
image: /assets/posts/2025-12-14-machine-learning-project/image.jpg
date: 2025-12-14 02:29:00 +09:00
last_modified_at: 2025-12-14 02:29:00 +09:00
---

이 프로젝트는 NLP를 **복잡한 모델링 영역**이 아니라  
**분류 문제로 환원했을 때 남는 최소 구조**를 고정하기 위한 기준점이다.

Naive Bayes 스팸 필터는 성능이 아니라  
**NLP 사고 흐름의 하한선**을 보여준다.

---

## 텍스트 분류 문제 정의

텍스트 분류의 본질은 단순하다.

- 입력: 문자열
- 출력: 클래스
- 핵심 문제: **문자를 어떻게 숫자로 바꿀 것인가**

이미지와 달리:
- 텍스트에는 공간 구조가 없다
- 순서 정보는 쉽게 깨진다
- 모델보다 **표현 방식**이 성패를 좌우한다

👉 NLP 분류는  
**“언어 문제”가 아니라 “표현 문제”**다.

---

## Count Vectorizer의 역할

### 왜 필요한가
- 모델은 문자열을 직접 처리하지 못한다
- 단어 출현을 **고정 길이 벡터**로 변환해야 한다
- 가장 단순한 방법이 빈도 기반 표현이다

역할 요약:
- 문서 → 단어 집합
- 단어 → 카운트
- 문서 → 숫자 벡터

👉 Count Vectorizer는  
**텍스트를 통계 문제로 바꾸는 장치**다.

### 무엇을 잃는지
- 단어 순서
- 문맥
- 의미적 유사성

즉:
- “좋다”와 “아주 좋다”의 차이
- “싫지 않다”의 반어적 의미  
이런 정보는 모두 사라진다.

👉 이 손실을 감수할 수 있을 때만 유효하다.

---

## Naive Bayes를 선택한 이유

### 이 문제에서 충분한 이유
- 스팸 필터는 **단어 존재 자체**가 강한 신호
- 단어 간 복잡한 상호작용이 필요 없음
- 빠르고 해석 가능

핵심 판단:
- 복잡한 모델이 필요한 문제가 아니다
- 데이터 표현이 이미 대부분의 정보를 담고 있다

👉 Naive Bayes는  
**“이 정도면 충분하다”의 기준선**이다.

---

## 이 구조의 한계

### 언제 깨지는가
- 문맥이 중요한 경우
- 단어 조합이 의미를 바꾸는 경우
- 표현이 중의적인 경우

대표 신호:
- 리뷰 감성 분석
- 풍자·반어
- 긴 문장 기반 판단

👉 이 한계를 넘는 순간,  
**문제는 더 이상 ‘단순 NLP 분류’가 아니다.**

---

## 최소 흐름 템플릿

```
text_data
→ vectorize(count)
→ train(classifier)
→ evaluate()
```

이 흐름이 통하지 않는다면:
- 모델이 아니라
- **문제 정의나 표현 방식**을 의심해야 한다.

---

- 참고: [Machine Learning 실전 개발 | 8개의 실용 프로젝트
](https://www.udemy.com/course/best-ml-8-real-project/)
