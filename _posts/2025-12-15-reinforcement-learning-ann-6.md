---
title: "왜 실무는 PPO / SAC인가: 안정성, 연속 제어, 엔트로피, 그리고 튜닝 현실"
description: "PPO와 SAC가 실무 표준이 된 이유와 연속 제어·안정성·튜닝 전략을 구조적으로 정리"
categories: ["🔁 Reinforcement Learning & ANN"]
tags: [PPO, SAC, ReinforcementLearning, ContinuousControl]
image: /assets/posts/2025-12-15-reinforcement-learning-ann/image.jpg
date: 2025-12-15 21:45:00 +09:00
last_modified_at: 2025-12-15 21:45:00 +09:00
---

# 왜 실무는 PPO / SAC인가: 안정성, 연속 제어, 엔트로피, 그리고 튜닝 현실

---

## 이 글의 목적

**핵심 요약**

* PPO와 SAC는 “이론적으로 멋진 알고리즘”이 아니라 **현장에서 버티는 알고리즘**이다.
* 실무 강화학습의 적은 성능 부족이 아니라 **불안정·붕괴·튜닝 지옥**이다.
* PPO는 *안정성*, SAC는 *연속 제어 + 샘플 효율*에서 강점을 가진다.
* 이 글은 “왜 다들 PPO/SAC를 쓰는지”를 구조적으로 설명한다.

**예시**

* 로봇 팔 제어에서 DQN이 바로 탈락하는 이유
* 같은 환경에서 PPO는 느리지만 안정, SAC는 빠르고 견고

---

## 1. PPO는 왜 등장했는가: 정책 업데이트 폭 제한

### 정의

* **PPO(Proximal Policy Optimization)** 는
  *“정책을 조금씩만 바꾸자”* 는 원칙을 코드로 구현한 알고리즘이다.

**핵심 요약**

* Policy Gradient 계열은 업데이트 한 번에 **정책이 망가질 수 있다**.
* TRPO는 이 문제를 해결했지만 구현이 너무 복잡했다.
* PPO는 **클리핑(clipping)** 으로 동일한 효과를 단순하게 달성한다.

### 직관

```text
어제 잘하던 정책이
오늘 한 번의 업데이트로 완전히 바뀌면?
→ 성능 붕괴 (policy collapse)
```

### 작동 원리 (직관 중심)

* 이전 정책과 **너무 멀어지는 업데이트는 잘라낸다**
* 결과적으로 “안전한 경사 하강”이 된다

**구현 포인트**

* 클리핑 계수(ε)가 안정성과 수렴 속도의 핵심
* Critic 품질이 낮으면 PPO도 무너진다

---

## 2. SAC의 핵심 아이디어: 최대 엔트로피 정책

### 정의

* **SAC(Soft Actor-Critic)** 는
  *보상을 최대화하면서 동시에 정책의 불확실성(엔트로피)을 유지*한다.

**핵심 요약**

* 기존 RL은 “가장 좋은 행동 하나”로 수렴하려 한다.
* SAC는 “여러 좋은 행동을 유지”한다.
* 결과는 **탐험이 자연스럽고 정책이 덜 깨진다**.

### 직관

```text
PPO: 제일 좋아 보이는 행동을 점점 굳힌다
SAC: 좋은 행동 후보들을 넓게 유지한다
```

### 왜 필요했나

* 연속 제어 문제는 국소 최적해가 많다.
* 엔트로피 항은 **조기 수렴과 탐험 부족을 구조적으로 방지**한다.

**구현 포인트**

* 온도 파라미터(α)는 자동 조정(auto-tuning)가 기본
* Q-network 2개(Double Q)는 과대평가 방지용

---

## 3. On-policy(PPO) vs Off-policy(SAC): 현실 차이

**핵심 요약**

* 이론보다 중요한 건 **데이터 재사용 가능 여부**다.
* PPO는 안정적이지만 데이터 낭비가 크다.
* SAC는 샘플 효율이 높아 **비싼 환경에서 유리**하다.

### 현실 비교

| 항목          | PPO (On-policy) | SAC (Off-policy) |
| ------------- | --------------- | ---------------- |
| 데이터 재사용 | 불가            | 가능             |
| 샘플 효율     | 낮음            | 높음             |
| 안정성        | 매우 높음       | 높음             |
| 튜닝 난이도   | 낮음            | 중               |
| 연속 행동     | 가능            | 매우 적합        |
| 실무 채택     | 광범위          | 로보틱스 중심    |

**결론 규칙**

* 환경이 싸고 단순 → **PPO**
* 환경이 비싸고 연속 제어 → **SAC**

---

## 4. 왜 연속 행동 공간이 중요한가 (자율주행·로보틱스)

**핵심 요약**

* 현실 세계는 대부분 **연속 제어 문제**다.
* 행동을 이산화하면 정밀도와 안정성이 동시에 깨진다.
* PPO/SAC는 연속 분포를 직접 다룬다.

### 예시

| 문제   | 이산 행동  | 연속 행동   |
| ------ | ---------- | ----------- |
| 조향각 | 좌/우/정지 | −30° ~ +30° |
| 가속   | 정지/전진  | 0.0 ~ 1.0   |

**결과**

* 이산화: 튀는 행동, 진동
* 연속: 부드러운 제어, 안정 주행

---

## 5. AWS DeepRacer 사례 재구성

### 문제 정의

* 자율주행 차량을 트랙에서 **빠르고 안정적으로 완주**

### MDP 구성

| 요소         | 정의                  |
| ------------ | --------------------- |
| 상태(State)  | 카메라 이미지, 속도   |
| 행동(Action) | 조향각, 가속 (연속)   |
| 보상(Reward) | 차선 유지, 속도, 완주 |
| 종료         | 트랙 이탈, 충돌       |

### 성공 조건

* 평균 랩 타임 감소
* 이탈 없는 완주 비율 증가
* 정책 안정성 유지

**왜 PPO/SAC인가**

* 연속 제어 필수
* 실시간 안정성 중요
* 보상 노이즈 존재 → PPO/SAC 유리

---

## 6. PPO vs SAC: 언제 무엇을 고를 것인가

### 최종 선택 표

| 상황                   | 추천 |
| ---------------------- | ---- |
| 빠른 구현, 안정 최우선 | PPO  |
| 로봇/자율주행          | SAC  |
| 데이터 비용 큼         | SAC  |
| 튜닝 인력 부족         | PPO  |
| 연속 + 복잡            | SAC  |

---

## 7. 보상 설계 체크리스트 (실무 핵심)

**핵심 요약**

* PPO/SAC 성능의 50%는 보상에서 결정된다.

### Reward Shaping 체크리스트

* [ ] 목표 행동이 즉시 보상으로 연결되는가
* [ ] 실패 행동에 명확한 패널티가 있는가
* [ ] 중간 보상이 최종 목표를 방해하지 않는가
* [ ] 보상 스케일이 과도하지 않은가
* [ ] sparse reward를 보조하는 신호가 있는가

---

## 8. 튜닝 실패 패턴과 처방

### 실패 1: 학습 불안정

* **증상**: reward 급등 후 붕괴
* **처방**:

  * PPO: clip ε ↓, lr ↓
  * SAC: α 자동 조정 확인

### 실패 2: 정책 collapse

* **증상**: 단일 행동 반복
* **처방**:

  * PPO: entropy bonus ↑
  * SAC: target smoothing 확인

### 실패 3: 과탐험 / 과수렴

* **증상**: 끝까지 랜덤 / 너무 빨리 고착
* **처방**:

  * PPO: entropy 스케줄링
  * SAC: reward 스케일 재조정

---

## 흔히 하는 오해 3가지와 교정

**오해 1**: “PPO는 느리다”
→ **교정**: 느린 게 아니라 **망가지지 않는다**.

**오해 2**: “SAC는 어렵다”
→ **교정**: 자동 α 튜닝으로 진입 장벽은 크게 낮아졌다.

**오해 3**: “둘 중 하나가 항상 더 낫다”
→ **교정**: 환경 비용과 행동 공간이 답을 정한다.

---

## 핵심 체크리스트 (재학습용)

* [ ] PPO의 클리핑 목적을 설명할 수 있는가
* [ ] SAC의 엔트로피 항 의미를 이해했는가
* [ ] On/Off-policy 차이를 데이터 관점에서 설명 가능한가
* [ ] 연속 행동이 필요한 문제를 구분할 수 있는가
* [ ] PPO vs SAC 선택 기준을 명확히 갖고 있는가
* [ ] 보상 설계가 정책을 잘 유도하는지 판단 가능한가
* [ ] 튜닝 실패 패턴을 증상만 보고 진단할 수 있는가

---

## 한 줄 요약

**PPO와 SAC가 실무 표준인 이유는 성능이 아니라, “깨지지 않고 끝까지 학습되는 구조”에 있다.**

---

- 참고: [ AI 만들기 2025: 강화학습과 인공신경망 완전정복, Agentic AI, Gen AI, RL
](https://www.udemy.com/course/best-ai-17-hours/)
