---
title: "퍼셉트론부터 역전파까지"
description: "이미지 분류 관점에서 퍼셉트론·활성화·손실·경사하강·역전파 직관을 정리"
categories: ["🥽 Computer Vision"]
tags: [Perceptron, Backpropagation, Activation, ImageClassification]
image: /assets/posts/2025-12-15-computer-vision/image.jpg
date: 2025-12-15 23:18:00 +09:00
last_modified_at: 2025-12-15 23:18:00 +09:00
---
## 문제 정의

**이미지 분류를 위한 신경망**은 이미지를 입력받아 “이 이미지가 무엇인가”라는 **하나의 라벨**을 예측하는 문제다.
핵심은 모델이 정답을 외우는 것이 아니라 **일반화된 판단 기준**을 학습하는 것이다.

**요약 한 줄**
신경망은 규칙을 코딩하는 기술이 아니라, **오차를 줄이는 방향을 학습하는 구조**다.

---

## 1) 퍼셉트론과 활성화 함수: 왜 선형 모델로는 부족한가

### 정의

* **퍼셉트론**: 입력의 가중합을 계산해 출력하는 가장 단순한 신경망
* **활성화 함수**: 가중합 결과를 비선형으로 변환하는 함수

### 직관

퍼셉트론만으로는 **직선 하나로 나눌 수 있는 문제**만 풀 수 있다.
이미지는 픽셀 수천 개가 얽힌 **비선형 문제**다.

활성화 함수는 “이 지점에서 꺾을 수 있게” 만들어
여러 층을 쌓아 **복잡한 경계**를 표현하게 한다.

### 오해 포인트

* 층을 많이 쌓으면 무조건 좋아진다
* 활성화 함수는 단순한 수학 장치다

→ 실제로는 **비선형성 확보가 목적**이다.

### 언제 쓰는가 / 쓰지 말아야 하는가

* ✔ 복잡한 패턴 분류
* ✘ 단순 선형 회귀 문제

### 구현 체크리스트

* 활성화 함수(ReLU 등) 누락 여부
* 출력층 활성화와 손실 함수의 조합 확인

### 요약

활성화 함수 없이는 신경망도 선형 모델에 불과하다.

---

## 2) 한 장의 이미지가 벡터로 바뀌는 순간

### 정의

Dense 신경망은 이미지를 **1차원 벡터**로 펼쳐 입력받는다.

### 직관

예를 들어 28×28 이미지는
→ 길이 784짜리 숫자 배열이 된다.

이 순간:

* 픽셀 간 **공간적 관계(이웃, 형태)** 는 사라진다
* 단지 “숫자 묶음”만 남는다

### 오해 포인트

* 이미지 정보가 그대로 유지된다고 착각
* Dense가 이미지 구조를 이해한다고 생각함

### 요약

Dense는 이미지를 “본다”기보다 **숫자를 계산**한다.

---

## 3) 손실(loss): 모델이 틀렸다는 신호

### 정의

**손실 함수**는 예측과 정답 사이의 **차이를 수치로 측정**한다.

### 직관

* 손실 = “얼마나 틀렸는가”
* 학습 = 이 값을 줄이는 과정

모델은 정답을 직접 보지 않는다.
오직 **손실 값 하나**만 보고 방향을 정한다.

### 오해 포인트

* 손실이 낮으면 항상 좋은 모델
* 훈련 손실만 보고 성능 판단

### 언제/언제 아님

* 훈련 손실 ↓ + 검증 손실 ↑ → 과적합 신호

### 구현 체크리스트

* 분류 문제에 맞는 손실 사용(categorical / sparse)
* 훈련·검증 손실 함께 모니터링

### 요약

손실은 신경망이 보는 **유일한 피드백**이다.

---

## 4) 경사하강법: 내리막을 찾는 방법

### 정의

경사하강법은 손실을 **가장 빠르게 줄이는 방향**으로 가중치를 조금씩 이동시키는 방법이다.

### 직관

* 산 정상에서 눈 감고 내려오는 상황
* 현재 기울기를 느끼고 가장 가파른 방향으로 한 걸음

**학습률(Learning Rate)** 은 그 한 걸음의 크기다.

### 오해 포인트

* 학습률이 클수록 빨리 수렴
  → 너무 크면 튕겨 나간다

### 구현 체크리스트

* 학습률 너무 크거나 작은지 확인
* 손실이 진동하면 학습률 의심

### 요약

학습률은 속도가 아니라 **안정성의 문제**다.

---

## 5) 역전파: 책임을 거꾸로 나누는 과정

### 정의

**역전파**는 출력 오류를 각 가중치의 **책임 비율**로 나누는 과정이다.

### 직관

* 결과가 틀렸다
* “이 레이어가 얼마나 잘못했는가?”를 뒤에서 앞으로 계산
* 책임이 큰 가중치는 크게 수정

수식을 몰라도 핵심은 이것이다:
**오차는 출력에서 입력 방향으로 분배된다.**

### 오해 포인트

* 역전파 = 복잡한 수학
* 직접 구현해야 이해 가능

### 요약

역전파는 계산이 아니라 **책임 배분 로직**이다.

---

## 6) Dense 신경망이 이미지에 취약한 이유

### 핵심 이유 2가지

1. **차원 폭발**

   * 픽셀 수 × 뉴런 수 → 파라미터 급증
2. **공간 구조 무시**

   * 인접 픽셀 정보 활용 불가

### 결과

* 데이터 많이 필요
* 작은 변화에도 취약
  → CNN이 등장한 이유

### 요약

Dense는 이미지에 “비효율적”이다.

---

## 과적합 · 일반화 · 검증셋

* **과적합**: 훈련 데이터만 잘 맞춤
* **일반화**: 처음 보는 데이터에도 성능 유지
* **검증셋**: 일반화 여부를 미리 확인하는 기준

핵심은 **훈련 성능이 아니라 검증 성능**이다.

---

## 핵심 코드 스니펫 (Dense 분류 예시)

```python
model = Sequential([
    Dense(256, activation='relu', input_shape=(784,)),  # 28x28 이미지 flatten
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')  # 클래스 수
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

**주의**

* 입력 shape는 반드시 flatten 이후 크기
* 출력층 활성화/손실 함수 짝 맞추기

---

## 최종 요약

* 신경망은 규칙이 아니라 **오차를 줄이는 구조**
* 활성화 함수가 비선형성을 만든다
* 역전파는 책임 분배다
* Dense는 이미지에 구조적으로 불리하다

---

## 다음 액션 플랜 (5줄)

1. Dense 신경망으로 이미지 분류를 직접 실험한다
2. 훈련/검증 손실 차이를 관찰한다
3. 과적합 지점을 확인한다
4. CNN이 필요한 이유를 체감한다
5. 다음 단계로 합성곱 신경망을 학습한다

---

- 참고: [Computer Vision (컴퓨터 비전) : 마스터 클래스
](https://www.udemy.com/course/best-computer-vision/)