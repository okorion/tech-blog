---
layout: post
title: "서비스 메시와 Istio가 필요해지는 순간"
description: "Istio를 도입할 시점과 트래픽·관측·보안 요구를 기준으로 한 판단 체크리스트"
categories: ["⛵ Istio & Service Mesh"]
tags: ["Istio", "Service Mesh", "Traffic Management", "Observability"]
image: /assets/posts/2025-12-31-k8s-istio/image.png
date: 2025-12-31 23:10:00 +09:00
---
## 결론 요약
Istio는 “쿠버네티스에서 뭔가 더 잘하고 싶어서” 도입하는 도구가 아니다.  
**서비스 간 통신이 운영 리스크의 중심이 되는 순간**에만 의미가 생긴다.  
트래픽 제어, 관측, 보안 요구가 **애플리케이션 코드 바깥에서** 강제되어야 할 때 Istio는 효과적이다.  
반대로 팀 규모·트래픽·장애 패턴이 단순한 상태에서 도입하면 비용만 증가한다.  
Istio는 생산성 도구가 아니라 **운영 복잡도를 구조적으로 떠안는 도구**다.  
따라서 “지금 못해서 불편한가”가 아니라 “지금 못해서 사고가 나는가”로 판단해야 한다.

---

## Kubernetes로 해결 가능한 범위

### 결론
**Kubernetes만으로도 ‘배포·확장·기본 트래픽 라우팅’은 충분히 가능하다.**

Kubernetes가 기본으로 제공하는 것:
- Service + kube-proxy: L4 로드밸런싱
- Deployment/ReplicaSet: 스케일링과 롤링 업데이트
- Ingress: 외부 → 내부 L7 라우팅
- ConfigMap/Secret: 설정 관리
- liveness/readiness probe: 최소한의 헬스 체크

작은 팀·작은 트래픽에서는 이 조합만으로도 운영이 된다.

### 예시 1: Istio가 필요 없는 케이스
- 팀 규모: 3~5명
- 서비스 수: 5개 이하
- 트래픽: 초당 수십~수백 RPS
- 장애 대응: “문제 생기면 로그 보고 고친다”
- 배포 전략: 단순 롤링 업데이트

이 상황에서 Istio는 **문제 해결이 아니라 복잡성 주입**이다.

---

## Kubernetes로 ‘운영’이 어려워지는 순간 (구체 시나리오)

### 결론
**문제가 ‘배포’가 아니라 ‘통신 정책과 가시성’으로 이동하면 한계가 드러난다.**

아래 상황이 동시에 발생하기 시작하면 Kubernetes 기본 기능만으로는 버겁다.

#### 시나리오 A: 트래픽 제어가 코드에 스며든다
- 카나리 배포를 하려면 각 서비스에 조건 분기 코드가 들어간다
- A/B 테스트 로직이 서비스마다 다르다
- 장애 시 특정 버전만 우회하고 싶은데 배포밖에 방법이 없다

#### 시나리오 B: 장애 원인을 모른다
- “느린 건 알겠는데 어디서 느린지 모른다”
- 서비스 A → B → C 중 어디서 타임아웃이 나는지 불명확
- 로그는 흩어져 있고, 트레이싱은 부분적으로만 있다

#### 시나리오 C: 보안 요구가 생긴다
- 서비스 간 통신 암호화(mTLS)가 필요해진다
- 인증/인가 정책을 서비스마다 구현하기 시작한다
- 내부 트래픽인데도 “누가 누구를 호출해도 되는지”를 통제해야 한다

이 시점부터 **운영 비용이 기하급수적으로 증가**한다.

---

## Istio가 제공하는 해결 축 (트래픽 / 관측 / 보안)

### 결론
**Istio는 서비스 간 통신 경로에 ‘공통 제어면’을 삽입한다.**

### 서비스 메시 정의 (어디에 끼어드는가)
Istio는 애플리케이션과 네트워크 사이,  
**Pod 옆(sidecar)에 프록시를 붙여 모든 통신을 강제 통과**시킨다.

```
App → Envoy(sidecar) → Network → Envoy → App
```

이 구조 덕분에 **코드 수정 없이** 다음을 제어한다.

#### 1) 트래픽
- 카나리/블루그린 배포
- 요청 비율 기반 라우팅
- 재시도, 타임아웃, 서킷 브레이커

#### 2) 관측
- 서비스 간 분산 트레이싱
- 표준화된 메트릭
- 통신 실패/지연의 위치 가시화

#### 3) 보안
- 서비스 간 mTLS 자동 적용
- 서비스 아이덴티티 기반 인증
- 네트워크 레벨 접근 제어

---

## 도입 비용과 트레이드오프

### 결론
**Istio는 ‘공짜 자동화’가 아니라 ‘비싼 운영 프레임워크’다.**

반드시 감수해야 할 비용:
- 학습 곡선: Envoy, VirtualService, DestinationRule 이해 필요
- 사이드카 오버헤드: CPU/메모리 증가
- 장애 복잡도: “애플리케이션 문제인지 메시 문제인지” 구분 필요
- 운영 난이도: 업그레이드, 설정 충돌, 디버깅 부담

즉, **운영 성숙도가 없으면 Istio가 장애 원인이 된다.**

---

## 도입 판단 기준 (체크리스트)

### 결론
아래 질문에 **Yes가 5개 이상**이면 검토 대상이다.

- 서비스 수가 10개 이상인가?
- 서비스 간 호출 경로를 한 눈에 설명하기 어려운가?
- 배포 전략(카나리/롤백)이 코드에 섞여 있는가?
- 장애 원인 분석에 평균 30분 이상 걸리는가?
- mTLS 또는 내부 인증 요구가 있는가?
- 서비스 간 SLA가 다르게 관리되어야 하는가?
- 트래픽 제어 정책을 중앙에서 통제하고 싶은가?
- 운영 전담 인력이 있는가?
- 쿠버네티스 네트워크 개념에 익숙한가?

---

## “도입하면 망하는” 신호 5가지

### 결론
아래 중 하나라도 해당하면 **지금은 아니다.**

1. “요즘 Istio 많이 쓰니까”
2. 장애 경험이 거의 없다
3. 모니터링/로깅도 아직 미정이다
4. 운영 담당이 따로 없다
5. 문제를 코드로 고치는 문화가 강하다

---

## 흔한 오해 3개 + 교정

1. **오해:** Istio = 성능 향상  
   **교정:** 성능은 대체로 나빠진다. 대신 통제력이 생긴다.

2. **오해:** Istio는 필수 인프라  
   **교정:** 필수 아니다. 특정 규모 이상에서만 유효하다.

3. **오해:** 한 번 도입하면 끝  
   **교정:** 지속적인 운영·튜닝 대상이다.

---

## 재학습 체크리스트

- 서비스 메시가 “네트워크 계층 문제”를 다룬다는 점을 이해했는가?
- Istio가 해결하는 문제를 현재 겪고 있는가?
- 트래픽/관측/보안 중 무엇이 가장 아픈가?
- 운영 인력이 감당 가능한가?
- 코드 외부에서 정책을 강제해야 할 이유가 있는가?
- 단순함을 포기할 준비가 되었는가?
- 대안(라이브러리, 경량 프록시)을 검토했는가?

---

## 한 줄 요약
**Istio는 ‘편해서’ 쓰는 도구가 아니라, 안 쓰면 사고가 나는 시점에만 쓰는 도구다.**

---

- 참고: [   Kubernetes Istio : 완벽 실습 과정
](https://www.udemy.com/course/kubernetes-istio-hands-on/)
