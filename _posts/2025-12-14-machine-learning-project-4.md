---
title: "LeNet으로 보는 CNN의 원형"
description: "LeNet을 통해 CNN의 기본 뼈대와 각 레이어 존재 이유를 복원하는 해설"
categories: ["🧠 Machine Learning Projects"]
tags: [CNN, LeNet, 이미지분류]
image: /assets/posts/2025-12-14-machine-learning-project/image.jpg
date: 2025-12-14 02:27:00 +09:00
last_modified_at: 2025-12-14 02:27:00 +09:00
---

이 프로젝트는 CNN을 **복잡한 모델 묶음**이 아니라  
**문제 해결 방식의 최소 단위**로 되돌리는 데 목적이 있다.

LeNet 기반 교통 신호 분류는  
“CNN이 왜 이런 형태로 굳어졌는가”를 가장 단순한 구조로 보여준다.

---

## LeNet 구조 요약

LeNet은 **CNN의 기본 뼈대**를 단계적으로 드러낸다.

- 입력 이미지
- Convolution
- Pooling
- Convolution
- Pooling
- Fully Connected
- 출력

구조적 특징을 bullet로 정리하면:

- **Conv → Pool 반복**
  - 국소 패턴 추출 후 요약
- **후반부 FC**
  - 공간 정보를 제거하고 결정에 집중
- **깊지 않은 구조**
  - 표현력보다 흐름이 명확

👉 CNN의 모든 변형은 이 틀에서 벗어나지 않는다.

---

## 왜 지금 봐도 의미 있는가

### 학습 목적 관점
- CNN의 **존재 이유를 구조만으로 설명 가능**
- 하이퍼파라미터·트릭에 시선이 분산되지 않는다
- “왜 이 레이어가 필요한가”에만 집중 가능

### 구조 단순성의 가치
- 각 레이어의 역할이 겹치지 않는다
- 문제 발생 시 원인 추적이 쉽다
- CNN 사고 흐름을 **기억 단위로 고정**하기 좋다

👉 LeNet은 성능이 아니라  
**이해를 위해 존재하는 구조**다.

---

## CIFAR-10과의 차이점

### 데이터 관점
- CIFAR-10: 일반 객체, 클래스 경계 모호
- 교통 신호: 형태 규칙 뚜렷, 시각적 패턴 명확

### 문제 난이도
- CIFAR-10: 고수준 특징 조합 필요
- 교통 신호: 저수준 패턴만으로도 분류 가능

### 모델 설계 관점
- CIFAR-10: 구조 선택이 성능에 영향
- LeNet 프로젝트: **구조 자체가 학습 대상**

👉 이 프로젝트의 핵심은  
“데이터가 단순하면 구조도 단순해진다”는 사실이다.

---

## 단순한 구조의 장점과 한계

### 언제 충분한가
- 입력 해상도가 낮을 때
- 패턴이 반복적이고 명확할 때
- 문제 복잡도가 제한적일 때

### 언제 부족한가
- 클래스 간 시각적 차이가 미묘할 때
- 다양한 스케일·배경 변화가 존재할 때
- 고수준 의미 추론이 필요할 때

👉 LeNet은 **CNN의 하한선**이다.  
이 선이 무너지는 지점이 곧 “더 복잡한 구조가 필요한 이유”다.

---

이 글의 역할은 명확하다.  
- CNN을 **이해 단위로 압축**
- 이후 어떤 CNN을 보더라도  
  → “LeNet에서 무엇이 추가되었는가”로 환원

여기까지 오면,  
CNN은 더 이상 블랙박스가 아니다.

---

- 참고: [Machine Learning 실전 개발 | 8개의 실용 프로젝트
](https://www.udemy.com/course/best-ml-8-real-project/)
